import json
from typing import List


from ..models import JobDBModel, LogMessageFlags, LogMessages, JobStatus
from ...celery_app import app
from ..analysis import Analyses


@app.task(name="dima")
def dima(sequences: str, job_id: str, kmer_length: int, header_format: List[str]) -> List[dict]:
    """
    Runs DiMA. Any kwargs will be directly passed to DiMA.

    :param sequences: The sequence to run through DiMA.
    :param job_id: The job id we are processing so that we can update the log.
    :param kmer_length: The length of kmers to generate via DiMA.
    :param header_format: The format of the header as provided by the user.

    :type sequences: str
    :type job_id: str
    :type kmer_length: int
    :type header_format: List[str]

    :returns: A dictionary of the kmer positions generated by DiMA
    """

    job_queryset = JobDBModel.objects.filter(id=job_id)
    results = None

    try:
        job_queryset.update_log(LogMessageFlags.INFO, LogMessages.RUN_MOTIF_CLASSIFICATION)
        results = Analyses.dima_analysis(sequences, kmer_length, header_format)
    except Exception:
        job_queryset.update_log(LogMessageFlags.ERROR, LogMessages.MOTIF_CLASSIFICATION_ERROR)
        job_queryset.update_status(JobStatus.FAILED)

        exit(1)

    job_queryset.update_log(LogMessageFlags.INFO, LogMessages.MOTIF_CLASSIFICATION_COMPLETE)
    # Unfortunate that we have to deserialize the results here. Alternative is to use the "pickle" option of Celery
    # which adds a lot of extra complexity. Or we could ditch the idea of parallel processing and use a single task.
    # Ultimately decided to opt for high throughput and go for parallel + simple.
    return json.loads(str(results.results))
